AdaptiveSparkPlan isFinalPlan=true
+- == Final Plan ==
   NativeTakeOrdered 100, [lochierarchy#1 DESC NULLS LAST, CASE WHEN (lochierarchy#1 = 0) THEN i_category#2 END ASC NULLS FIRST, rank_within_parent#3 ASC NULLS FIRST]
   +- NativeProject [total_sum#4, i_category#2, i_class#5, lochierarchy#1, rank_within_parent#3]
      +- NativeWindow [rank(_w0#6) windowspecdefinition(_w1#7, _w2#8, _w0#6 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank_within_parent#3], [_w1#7, _w2#8], [_w0#6 DESC NULLS LAST]
         +- NativeSort [_w1#7 ASC NULLS FIRST, _w2#8 ASC NULLS FIRST, _w0#6 DESC NULLS LAST], false
            +- InputAdapter
               +- AQEShuffleRead coalesced
                  +- ShuffleQueryStage 5
                     +- NativeShuffleExchange hashpartitioning(_w1#7, _w2#8, 100), ENSURE_REQUIREMENTS, [plan_id=1]
                        +- NativeProject [MakeDecimal(sum(UnscaledValue(ws_net_paid#9))#10,17,2) AS total_sum#4, i_category#2, i_class#5, (cast((shiftright(spark_grouping_id#11, 1) & 1) as tinyint) + cast((shiftright(spark_grouping_id#11, 0) & 1) as tinyint)) AS lochierarchy#1, MakeDecimal(sum(UnscaledValue(ws_net_paid#9))#10,17,2) AS _w0#6, (cast((shiftright(spark_grouping_id#11, 1) & 1) as tinyint) + cast((shiftright(spark_grouping_id#11, 0) & 1) as tinyint)) AS _w1#7, CASE WHEN (cast((shiftright(spark_grouping_id#11, 0) & 1) as tinyint) = 0) THEN i_category#2 END AS _w2#8]
                           +- NativeHashAggregate HashAgg, List(i_category#2, i_class#5, spark_grouping_id#11), [i_category#2, i_class#5, spark_grouping_id#11], [sum(UnscaledValue(ws_net_paid#9))], [sum(UnscaledValue(ws_net_paid#9))#10], 3
                              +- InputAdapter
                                 +- AQEShuffleRead coalesced
                                    +- ShuffleQueryStage 4
                                       +- NativeShuffleExchange hashpartitioning(i_category#2, i_class#5, spark_grouping_id#11, 100), ENSURE_REQUIREMENTS, [plan_id=2]
                                          +- NativeHashAggregate HashAgg, [i_category#2, i_class#5, spark_grouping_id#11], [partial_sum(_c3#12)], [sum#13], 0
                                             +- NativeProject [i_category#2 AS i_category#2, i_class#5 AS i_class#5, spark_grouping_id#11 AS spark_grouping_id#11, UnscaledValue(ws_net_paid#9) AS _c3#12]
                                                +- !NativeExpand [[ws_net_paid#9, i_category#14, i_class#15, 0], [ws_net_paid#9, i_category#14, null, 1], [ws_net_paid#9, null, null, 3]], [ws_net_paid#9, i_category#2, i_class#5, spark_grouping_id#11]
                                                   +- NativeProject [ws_net_paid#9, i_category#14, i_class#15]
                                                      +- NativeSortMergeJoin [ws_item_sk#16], [i_item_sk#17], Inner
                                                         :- NativeSort [ws_item_sk#16 ASC NULLS FIRST], false
                                                         :  +- InputAdapter
                                                         :     +- AQEShuffleRead coalesced
                                                         :        +- ShuffleQueryStage 3
                                                         :           +- NativeShuffleExchange hashpartitioning(ws_item_sk#16, 100), ENSURE_REQUIREMENTS, [plan_id=3]
                                                         :              +- NativeProject [ws_item_sk#16, ws_net_paid#9]
                                                         :                 +- NativeSortMergeJoin [ws_sold_date_sk#18], [d_date_sk#19], Inner
                                                         :                    :- NativeSort [ws_sold_date_sk#18 ASC NULLS FIRST], false
                                                         :                    :  +- InputAdapter
                                                         :                    :     +- AQEShuffleRead coalesced
                                                         :                    :        +- ShuffleQueryStage 0
                                                         :                    :           +- NativeShuffleExchange hashpartitioning(ws_sold_date_sk#18, 100), ENSURE_REQUIREMENTS, [plan_id=4]
                                                         :                    :              +- NativeFilter (isnotnull(ws_sold_date_sk#18) AND isnotnull(ws_item_sk#16))
                                                         :                    :                 +- InputAdapter [#18, #16, #9]
                                                         :                    :                    +- NativeParquetScan  (FileScan parquet [ws_sold_date_sk#18,ws_item_sk#16,ws_net_paid#9] Batched: true, DataFilters: [isnotnull(ws_sold_date_sk#18), isnotnull(ws_item_sk#16)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/<warehouse_dir>], PartitionFilters: [], PushedFilters: [IsNotNull(ws_sold_date_sk), IsNotNull(ws_item_sk)], ReadSchema: struct<ws_sold_date_sk:int,ws_item_sk:int,ws_net_paid:decimal(7,2)>)
                                                         :                    +- NativeSort [d_date_sk#19 ASC NULLS FIRST], false
                                                         :                       +- InputAdapter
                                                         :                          +- AQEShuffleRead coalesced
                                                         :                             +- ShuffleQueryStage 1
                                                         :                                +- NativeShuffleExchange hashpartitioning(d_date_sk#19, 100), ENSURE_REQUIREMENTS, [plan_id=5]
                                                         :                                   +- NativeProject [d_date_sk#19]
                                                         :                                      +- NativeFilter (((isnotnull(d_month_seq#20) AND (d_month_seq#20 >= 1200)) AND (d_month_seq#20 <= 1211)) AND isnotnull(d_date_sk#19))
                                                         :                                         +- InputAdapter [#19, #20]
                                                         :                                            +- NativeParquetScan  (FileScan parquet [d_date_sk#19,d_month_seq#20] Batched: true, DataFilters: [isnotnull(d_month_seq#20), (d_month_seq#20 >= 1200), (d_month_seq#20 <= 1211), isnotnul..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/<warehouse_dir>], PartitionFilters: [], PushedFilters: [IsNotNull(d_month_seq), GreaterThanOrEqual(d_month_seq,1200), LessThanOrEqual(d_month_seq,1211),..., ReadSchema: struct<d_date_sk:int,d_month_seq:int>)
                                                         +- NativeSort [i_item_sk#17 ASC NULLS FIRST], false
                                                            +- InputAdapter
                                                               +- AQEShuffleRead coalesced
                                                                  +- ShuffleQueryStage 2
                                                                     +- NativeShuffleExchange hashpartitioning(i_item_sk#17, 100), ENSURE_REQUIREMENTS, [plan_id=6]
                                                                        +- NativeFilter isnotnull(i_item_sk#17)
                                                                           +- InputAdapter [#17, #15, #14]
                                                                              +- NativeParquetScan  (FileScan parquet [i_item_sk#17,i_class#15,i_category#14] Batched: true, DataFilters: [isnotnull(i_item_sk#17)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/<warehouse_dir>], PartitionFilters: [], PushedFilters: [IsNotNull(i_item_sk)], ReadSchema: struct<i_item_sk:int,i_class:string,i_category:string>)
+- == Initial Plan ==
   TakeOrderedAndProject(limit=100, orderBy=[lochierarchy#1 DESC NULLS LAST,CASE WHEN (lochierarchy#1 = 0) THEN i_category#2 END ASC NULLS FIRST,rank_within_parent#3 ASC NULLS FIRST], output=[total_sum#4,i_category#2,i_class#5,lochierarchy#1,rank_within_parent#3])
   +- Project [total_sum#4, i_category#2, i_class#5, lochierarchy#1, rank_within_parent#3]
      +- Window [rank(_w0#6) windowspecdefinition(_w1#7, _w2#8, _w0#6 DESC NULLS LAST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank_within_parent#3], [_w1#7, _w2#8], [_w0#6 DESC NULLS LAST]
         +- Sort [_w1#7 ASC NULLS FIRST, _w2#8 ASC NULLS FIRST, _w0#6 DESC NULLS LAST], false, 0
            +- Exchange hashpartitioning(_w1#7, _w2#8, 100), ENSURE_REQUIREMENTS, [plan_id=7]
               +- HashAggregate(keys=[i_category#2, i_class#5, spark_grouping_id#11], functions=[sum(UnscaledValue(ws_net_paid#9))], output=[total_sum#4, i_category#2, i_class#5, lochierarchy#1, _w0#6, _w1#7, _w2#8])
                  +- Exchange hashpartitioning(i_category#2, i_class#5, spark_grouping_id#11, 100), ENSURE_REQUIREMENTS, [plan_id=8]
                     +- HashAggregate(keys=[i_category#2, i_class#5, spark_grouping_id#11], functions=[partial_sum(UnscaledValue(ws_net_paid#9))], output=[i_category#2, i_class#5, spark_grouping_id#11, sum#21])
                        +- Expand [[ws_net_paid#9, i_category#14, i_class#15, 0], [ws_net_paid#9, i_category#14, null, 1], [ws_net_paid#9, null, null, 3]], [ws_net_paid#9, i_category#2, i_class#5, spark_grouping_id#11]
                           +- Project [ws_net_paid#9, i_category#14, i_class#15]
                              +- SortMergeJoin [ws_item_sk#16], [i_item_sk#17], Inner
                                 :- Sort [ws_item_sk#16 ASC NULLS FIRST], false, 0
                                 :  +- Exchange hashpartitioning(ws_item_sk#16, 100), ENSURE_REQUIREMENTS, [plan_id=9]
                                 :     +- Project [ws_item_sk#16, ws_net_paid#9]
                                 :        +- SortMergeJoin [ws_sold_date_sk#18], [d_date_sk#19], Inner
                                 :           :- Sort [ws_sold_date_sk#18 ASC NULLS FIRST], false, 0
                                 :           :  +- Exchange hashpartitioning(ws_sold_date_sk#18, 100), ENSURE_REQUIREMENTS, [plan_id=10]
                                 :           :     +- Filter (isnotnull(ws_sold_date_sk#18) AND isnotnull(ws_item_sk#16))
                                 :           :        +- FileScan parquet [ws_sold_date_sk#18,ws_item_sk#16,ws_net_paid#9] Batched: true, DataFilters: [isnotnull(ws_sold_date_sk#18), isnotnull(ws_item_sk#16)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/<warehouse_dir>], PartitionFilters: [], PushedFilters: [IsNotNull(ws_sold_date_sk), IsNotNull(ws_item_sk)], ReadSchema: struct<ws_sold_date_sk:int,ws_item_sk:int,ws_net_paid:decimal(7,2)>
                                 :           +- Sort [d_date_sk#19 ASC NULLS FIRST], false, 0
                                 :              +- Exchange hashpartitioning(d_date_sk#19, 100), ENSURE_REQUIREMENTS, [plan_id=11]
                                 :                 +- Project [d_date_sk#19]
                                 :                    +- Filter (((isnotnull(d_month_seq#20) AND (d_month_seq#20 >= 1200)) AND (d_month_seq#20 <= 1211)) AND isnotnull(d_date_sk#19))
                                 :                       +- FileScan parquet [d_date_sk#19,d_month_seq#20] Batched: true, DataFilters: [isnotnull(d_month_seq#20), (d_month_seq#20 >= 1200), (d_month_seq#20 <= 1211), isnotnul..., Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/<warehouse_dir>], PartitionFilters: [], PushedFilters: [IsNotNull(d_month_seq), GreaterThanOrEqual(d_month_seq,1200), LessThanOrEqual(d_month_seq,1211),..., ReadSchema: struct<d_date_sk:int,d_month_seq:int>
                                 +- Sort [i_item_sk#17 ASC NULLS FIRST], false, 0
                                    +- Exchange hashpartitioning(i_item_sk#17, 100), ENSURE_REQUIREMENTS, [plan_id=12]
                                       +- Filter isnotnull(i_item_sk#17)
                                          +- FileScan parquet [i_item_sk#17,i_class#15,i_category#14] Batched: true, DataFilters: [isnotnull(i_item_sk#17)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/<warehouse_dir>], PartitionFilters: [], PushedFilters: [IsNotNull(i_item_sk)], ReadSchema: struct<i_item_sk:int,i_class:string,i_category:string>

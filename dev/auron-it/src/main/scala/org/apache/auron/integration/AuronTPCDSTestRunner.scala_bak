/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.auron.integration

import org.apache.commons.io.FileUtils
import org.apache.spark.SparkConf
import org.apache.spark.sql.execution.FormattedMode
import org.apache.spark.sql.types.DoubleType
import org.apache.spark.sql.{DataFrame, Row, SparkSession}

import java.io.File
import java.nio.charset.StandardCharsets
import scala.collection.mutable
import scala.util.matching.Regex

abstract class AuronTPCDSSuite {
  protected val regenGoldenFiles: Boolean =
    sys.env.getOrElse("REGEN_TPCDS_GOLDEN_FILES", "0") == "1"

  protected val tpcdsDataPath: String =
    //sys.env.getOrElse("SPARK_TPCDS_DATA", "")
    sys.env.getOrElse("SPARK_TPCDS_DATA", "/Users/yew1eb/workspaces/tpcds-validator/tpcds_1g")

  protected val execTpcdsQueries: String =
    sys.env.getOrElse("SPARK_TPCDS_QUERY", "")

  protected val tpcdsExtraSparkConfs: String =
    sys.env.getOrElse(
      "SPARK_TPCDS_EXTRA_CONF",
      "--conf spark.auron.ui.enabled=false  --conf spark.serializer=org.apache.spark.serializer.KryoSerializer")

  protected val rootPath: String = getClass.getResource("/").getPath
  protected val tpcdsQueriesPath: String = rootPath + "/tpcds-queries"
  protected val tpcdsResultsPath: String = rootPath + "/tpcds-query-results"
  protected val tpcdsPlanPath: String = rootPath + "/tpcds-plan-stability"

  protected val RE_GENERATE: Boolean = (sys.env.getOrElse("SPARK_TPCDS_REGENERATE", "0") == "1")

  protected val colSep: String = "<|COL|>"

  val tpcdsQueries: Seq[String] = Seq(
    "q1",
    "q2",
    "q3",
    "q4",
    "q5",
    "q6",
    "q7",
    "q8",
    "q9",
    "q10",
    "q11",
    "q12",
    "q13",
    "q14a",
    "q14b",
    "q15",
    "q16",
    "q17",
    "q18",
    "q19",
    "q20",
    "q21",
    "q22",
    "q23a",
    "q23b",
    "q24a",
    "q24b",
    "q25",
    "q26",
    "q27",
    "q28",
    "q29",
    "q30",
    "q31",
    "q32",
    "q33",
    "q34",
    "q35",
    "q36",
    "q37",
    "q38",
//    "q39a",
//    "q39b",
    "q40",
    "q41",
    "q42",
    "q43",
    "q44",
    "q45",
    "q46",
    "q47",
    "q48",
    "q49",
    "q50",
    "q51",
    "q52",
    "q53",
    "q54",
    "q55",
    "q56",
    "q57",
    "q58",
    "q59",
    "q60",
    "q61",
    "q62",
    "q63",
    "q64",
    "q65",
    "q66",
    "q67",
    "q68",
    "q69",
    "q70",
    "q71",
    "q72",
    "q73",
    "q74",
    "q75",
    "q76",
    "q77",
    "q78",
    "q79",
    "q80",
    "q81",
    "q82",
    "q83",
    "q84",
    "q85",
    "q86",
    "q87",
    "q88",
    "q89",
    "q90",
    "q91",
    "q92",
    "q93",
    "q94",
    "q95",
    "q96",
    "q97",
    "q98",
    "q99")

  protected var queryTables: Map[String, DataFrame] = _

  var needRunQueries: Seq[String] = Seq.empty
  var joinConfs: Map[String, String] = Map.empty



  private def filterQueries(
      origQueries: Seq[String],
      queryFilter: Set[String],
      nameSuffix: String = ""): Seq[String] = {
    if (queryFilter.nonEmpty) {
      if (nameSuffix.nonEmpty) {
        origQueries.filter { name => queryFilter.contains(s"$name$nameSuffix") }
      } else {
        origQueries.filter(queryFilter.contains)
      }
    } else {
      origQueries
    }
  }

  override protected def sparkConf: SparkConf = {
    val baseConf = super.sparkConf
      .set("spark.sql.extensions", "org.apache.spark.sql.auron.AuronSparkSessionExtension")
      .set(
        "spark.shuffle.manager",
        "org.apache.spark.sql.execution.auron.shuffle.AuronShuffleManager")
      .set("spark.memory.offHeap.enabled", "false")
      .set("spark.ui.enabled", "false")
      .set("spark.auron.ui.enabled", "false")
      .set("spark.auron.enable", "true")

    joinConfs.foreach { case (key, value) =>
      baseConf.set(key, value)
    }

    baseConf
  }

  override def beforeAll(): Unit = {
    super.beforeAll()
    spark.sparkContext.setLogLevel("WARN")
    createQueryTables()
  }

  protected def createQueryTables(): Unit = {
    queryTables = Seq(
      "call_center",
      "catalog_page",
      "catalog_returns",
      "catalog_sales",
      "customer",
      "customer_address",
      "customer_demographics",
      "date_dim",
      "household_demographics",
      "income_band",
      "inventory",
      "item",
      "promotion",
      "reason",
      "ship_mode",
      "store",
      "store_returns",
      "store_sales",
      "time_dim",
      "warehouse",
      "web_page",
      "web_returns",
      "web_sales",
      "web_site").map { tableName =>
      val tablePath = new File(tpcdsDataPath, tableName).getAbsolutePath
      val tableDF = spark.read.format("parquet").load(tablePath)
      tableDF.createOrReplaceTempView(tableName)
      (tableName, tableDF)
    }.toMap
  }

  def shouldVerifyPhysicalPlan(): Boolean = {
    Shims.get.shimVersion match {
      case "spark-3.5" => true
      case _ => false // TODO: Support for other Spark versions in the future
    }
  }

  protected def checkQueryResult(df: DataFrame, queryId: String): Unit = {
    val goldenFile = new File(s"$tpcdsResultsPath/$queryId.out")
    val rows = df.collect()

    if (regenGoldenFiles) {
      writeGoldenFile(goldenFile, formatResultContent(rows))
      return
    }

    if (df.schema.exists(_.dataType == DoubleType)) {
      compareDoubleResult(queryId, rows, goldenFile)
    } else {
      compareResultStr(queryId, rows, goldenFile)
    }
  }

  private def formatResultContent(rows: Array[Row]): String = {
    val rowStrings = rows.map(_.mkString(colSep))
    s"${rows.length}\n${rowStrings.mkString("\n")}\n"
  }

  private def writeGoldenFile(file: File, content: String): Unit = {
    Option(file.getParentFile).foreach(_.mkdirs())
    FileUtils.writeStringToFile(file, content, StandardCharsets.UTF_8)
  }

  protected def compareResultStr(sqlNum: String, rows: Array[Row], goldenFile: File): Unit = {
    val actualContent = formatResultContent(rows)

    val expectedResult = FileUtils.readFileToString(goldenFile, StandardCharsets.UTF_8)
    if (expectedResult != actualContent) {
      fail(s"""
              |=== $sqlNum result does NOT match expected ===
              |[Expected]
              |${expectedResult}
              |[Actual]
              |${actualContent}
              |""".stripMargin)
    }
  }

  protected def compareDoubleResult(
      queryId: String,
      rows: Array[Row],
      goldenFile: File,
      tolerance: Double = 1e-6): Unit = {

    val expectedRowIter = FileUtils.readLines(goldenFile, StandardCharsets.UTF_8).iterator()
    val expectedRowCount = expectedRowIter.next().toInt
    assert(
      rows.length == expectedRowCount,
      s"Row count mismatch in $queryId: expected $expectedRowCount, got ${rows.length}")

    rows.zipWithIndex.foreach { case (actualRow, rowIdx) =>
      assert(expectedRowIter.hasNext)
      val expectedRow = expectedRowIter.next().split(Regex.quote(colSep))

      actualRow.schema.zipWithIndex.foreach { case (field, colIdx) =>
        if (actualRow.isNullAt(colIdx)) {
          assert("null".equals(expectedRow(colIdx)))
        } else {
          field.dataType match {
            case DoubleType =>
              val actualValue = actualRow.getDouble(colIdx)
              val expectedValue = expectedRow(colIdx).toDouble
              val diff = math.abs(actualValue - expectedValue)
              assert(
                diff < tolerance,
                s"Floating-point mismatch in $queryId row $rowIdx col $colIdx: " +
                  s"expected ${expectedValue}, got ${actualValue} (diff=$diff)")
            case _ =>
              val actualValue = actualRow.get(colIdx).toString
              val expectedValue = expectedRow(colIdx)
              assert(
                actualValue == expectedValue,
                s"Mismatch in $queryId row $rowIdx col $colIdx: " +
                  s"expected $expectedValue, got $actualValue")
          }
        }
      }
    }
  }

  private def normalizePhysicalPlan(plan: String): String = {
    val exprIdRegex = "#\\d+L?".r
    val planIdRegex = "plan_id=\\d+".r

    // Normalize file location
    def normalizeLocation(plan: String): String = {
      plan.replaceAll("""file:/[^,\s\]\)]+""", "file:/<warehouse_dir>")
    }

    // Create a normalized map for regex matches
    def createNormalizedMap(regex: Regex, plan: String): Map[String, String] = {
      val map = new mutable.HashMap[String, String]()
      regex
        .findAllMatchIn(plan)
        .map(_.toString)
        .foreach(map.getOrElseUpdate(_, (map.size + 1).toString))
      map.toMap
    }

    // Replace occurrences in the plan using the normalized map
    def replaceWithNormalizedValues(
        plan: String,
        regex: Regex,
        normalizedMap: Map[String, String],
        format: String): String = {
      regex.replaceAllIn(plan, regexMatch => s"$format${normalizedMap(regexMatch.toString)}")
    }

    // Normalize the entire plan step by step
    val exprIdMap = createNormalizedMap(exprIdRegex, plan)
    val exprIdNormalized = replaceWithNormalizedValues(plan, exprIdRegex, exprIdMap, "#")

    val planIdMap = createNormalizedMap(planIdRegex, exprIdNormalized)
    val planIdNormalized =
      replaceWithNormalizedValues(exprIdNormalized, planIdRegex, planIdMap, "plan_id=")

    // QueryStageExec will take its id as argument, replace it with X
    val argumentsNormalized = planIdNormalized
      .replaceAll("Arguments: [0-9]+, [0-9]+", "Arguments: X, X")
      .replaceAll("Arguments: [0-9]+", "Arguments: X")

    normalizeLocation(argumentsNormalized)
  }

  protected def checkPhysicalPlan(df: DataFrame, queryId: String): Unit = {
    if (!shouldVerifyPhysicalPlan()) {
      return
    }

    val goldenPlanFile = new File(s"$tpcdsPlanPath/$queryId.txt")
    val actualPlan = normalizePhysicalPlan(df.queryExecution.explainString(FormattedMode))

    if (regenGoldenFiles) {
      writeGoldenFile(goldenPlanFile, actualPlan)
      return
    }

    val expectedPlan = FileUtils.readFileToString(goldenPlanFile, StandardCharsets.UTF_8)
    if (expectedPlan != actualPlan) {
      val actualTempFile = new File(FileUtils.getTempDirectory, s"tpch.actual.plan.$queryId.txt")
      FileUtils.writeStringToFile(actualTempFile, actualPlan, StandardCharsets.UTF_8)
      fail(s"""
              |Physical plan mismatch for query $queryId
              |Expected: ${goldenPlanFile.getAbsolutePath}
              |Actual  : ${actualTempFile.getAbsolutePath}
              |
              |--- Expected ---
              |$expectedPlan
              |
              |--- Actual ---
              |$actualPlan
              |""".stripMargin)
    }
  }

  needRunQueries.foreach { queryId =>
    test(s"TPC-DS $queryId") {
      val queryFile = new File(s"$tpcdsQueriesPath/$queryId.sql")
      val sqlText = FileUtils.readFileToString(queryFile, StandardCharsets.UTF_8).trim

      val resultDf = spark.sql(sqlText)

      checkQueryResult(resultDf, queryId)
      checkPhysicalPlan(resultDf, queryId)
    }
  }

}

class AuronTPCDSV1Suite extends AuronTPCDSSuite {
  override protected def sparkConf: SparkConf = {
    super.sparkConf
      .set("spark.sql.sources.useV1SourceList", "parquet")
      .set("spark.sql.autoBroadcastJoinThreshold", "-1")
  }

  case class Config(
                     query: Option[String] = None,
                     dataPath: String = "/tmp/tpcds_1g",
                     scale: Int = 1,
                     extraConf: Map[String, String] = Map.empty
                   )

  def main(args: Array[String]): Unit = {
    val parser = new scopt.OptionParser[Config]("AuronTPCDSSuite") {
      head("Auron TPC-DS Integration Test Runner", "1.0")
      opt[String]("query") action { (x, c) => c.copy(query = Some(x)) } text "Single TPC-DS query ID (e.g., q1, q42)"
      opt[String]("data") action { (x, c) => c.copy(dataPath = x) } text "Path to TPC-DS data directory"
      opt[Int]("scale") action { (x, c) => c.copy(scale = x) } text "TPC-DS scale factor (default: 1)"
      help("help") text "Print this help message"
    }

    parser.parse(args, Config()) match {
      case Some(config) => runTests(config)
      case None => sys.exit(1)
    }
  }

  private def runTests(config: Config): Unit = {
    println("=== Auron TPC-DS Integration Test Runner ===")
    println(s"Target query: ${config.query.getOrElse("all (99 queries)")}")
    println(s"Data path: ${config.dataPath}")
    println(s"Scale factor: ${config.scale}")

    val spark = SparkSession.builder()
      .appName("AuronTpcdsITRunner")
      .master("local[*]")
      .getOrCreate()

    // 支持 CI 传入额外 Spark 配置（如 shuffle backend）
    Option(System.getenv("SPARK_TPCDS_EXTRA_CONF")).foreach { confStr =>
      confStr.split(",").foreach { kv =>
        if (kv.contains("=")) {
          val Array(k, v) = kv.split("=", 2)
          spark.conf.set(k.trim, v.trim)
        }
      }
    }

    spark.sparkContext.setLogLevel("WARN")
    spark.sql(s"USE tpcds_sf${config.scale}")

    val queries = TpcdsQueries.allQueries
    val queriesToRun = config.query match {
      case Some(qid) if queries.contains(qid) => Map(qid -> queries(qid))
      case Some(qid) => sys.error(s"Unknown query ID: $qid")
      case None => queries
    }

    queriesToRun.foreach { case (qid, sqlText) =>
      println(s"[$qid] Executing TPC-DS query...")

      val df = spark.sql(sqlText)

      // 基本结果验证（行数）
      val rowCount = df.count()
      println(s"[$qid] Result row count: $rowCount")

      // 核心：Auron 计划稳定性检查
      val physicalPlan = df.queryExecution.executedPlan.toString().toLowerCase

      if (!physicalPlan.contains("auron") && !physicalPlan.contains("columnar")) {
        System.err.println(s"[$qid] FAILED: No Auron native operator detected in physical plan!")
        System.err.println(s"Plan snippet:\n${df.queryExecution.executedPlan}")
        sys.exit(1)
      }

      println(s"[$qid] PASSED: Auron native acceleration successfully applied")
    }

    spark.stop()
    println("=== All TPC-DS integration tests passed successfully ===")
  }
}
}
